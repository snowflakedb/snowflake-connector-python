{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake + Dask\n",
    "\n",
    "How to load data from a Snowflake table or query into a Dask dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Snowflake\n",
    "\n",
    "See [README](README.md) for more details on how to set up the credentials file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import snowflake.connector\n",
    "\n",
    "creds = yaml.full_load(open('/home/jovyan/snowflake_creds.yml'))\n",
    "\n",
    "# get connection info\n",
    "conn_info = {\n",
    "    'warehouse': 'COMPUTE_WH',\n",
    "    'database': 'NYC_TAXI',\n",
    "    'schema': 'PUBLIC',\n",
    "    **creds,\n",
    "}\n",
    "conn = snowflake.connector.connect(**conn_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup query\n",
    "\n",
    "We need to set up a query that will return chunks of the full result based on a column in the data. These become our partitions in a Dask dataframe. We use a [binding for the Snowflake query](https://docs.snowflake.com/en/user-guide/python-connector-example.html#binding-data) so that we can pass different values at execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM taxi_yellow\n",
    "WHERE\n",
    "    date_trunc('MONTH', tpep_pickup_datetime) = '2020-01-01'\n",
    "    AND day(tpep_pickup_datetime) = %s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the query is good with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor().execute(query, 1)\n",
    "df = cur.fetch_pandas_all()\n",
    "len(df), df.memory_usage().sum() / 1e6  # memory size in MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait\n",
    "from dask_saturn import SaturnCluster\n",
    "import time\n",
    "\n",
    "n_workers = 3\n",
    "cluster = SaturnCluster(n_workers=n_workers, scheduler_size='medium', worker_size='large', nthreads=2)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you initialized your cluster here in this notebook, it might take a few minutes for all your nodes to become available. You can run the chunk below to block until all nodes are ready\n",
    "\n",
    "> **Pro tip:** Create and/or start your cluster from the \"Dask\" page in Saturn if you want to get a head start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(client.scheduler_info()['workers']) < n_workers:\n",
    "    print('Waiting for workers, got', len(client.scheduler_info()['workers']))\n",
    "    time.sleep(30)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load larger data with Dask!\n",
    "\n",
    "We set up a function with `dask.delayed`. `@delayed` is a decorator that turns a Python function into a function suitable for running on the Dask cluster. When you execute a delayed function, instead of executing the operation, it returns a delayed result that represents what the return value of the function will be. `dask.dataframe.from_delayed` takes a list of these delayed objects, and concatenates them into a Dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def load(conn_info, query, day):\n",
    "    conn = snowflake.connector.connect(**conn_info)\n",
    "    cur = conn.cursor().execute(query, day)\n",
    "    return cur.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now `ddf` has 3 partitions, corresponding to the 3 days we chose to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_delayed([load(conn_info, query, day) for day in range(1, 32)])\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "`repartition()` to introduce more parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.repartition(100)\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The cell below will execute the Snowflake queries across the cluster and compute the length and size of each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ddf), ddf.memory_usage().sum().compute() / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partitions in the Dask dataframe are pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_part = ddf.partitions[0].compute()\n",
    "type(ddf_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are performing a lot of operations using this Dask dataframe (such as training a machine learning model), and the data will fit in the memory of the _cluster_, we should `persist()` the dataframe to perform all the loading up-front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "\n",
    "ddf = ddf.persist()\n",
    "_ = wait(ddf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
